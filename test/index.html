<html>
	<head> <title> Hypotheses! </title>
	</head>
	<body>
		What kind of problem is software development?

		The construction of robust software is notoriously difficult. We have a certain perplexity about it too, since we see other engineering disciplines performing so much more reliably. And we know it’s more complex, so of course we’ll make more mistakes—but I wonder if becoming more disciplined engineers is really the solution. The proposal of this essay is that it may be a damaging error in perspective to think of it as a task of engineering in the first place, when the methodology of science (1) may be more appropriate. I’ll start with an abstract overview of my argument, hopefully establish its veracity in the several paragraphs following, and then finish with some speculations on how we could improve our methodology.

		————————————————

		The appropriate methodology for a discipline is determined by the character of the problems within the discipline; the character of problems in software development shares more with the character of problems in science than in engineering. The main objective in science and software is to develop a precise description of some phenomenon, which happens to be physical in science and mental in software (and mathematics: http://plato.stanford.edu/entries/platonism-mathematics). Toward this end, a process of hypothesis generation, and subsequent affirmation or rejection by empirical verification (using a debugger or similar in software), guides refinements and further developments of the descriptions formulated. Additionally, software development’s present methodology already possesses the core features of scientific methodology, but expresses them in a primitive fashion.

		In both disciplines the highest level split in activity types forms two categories, one having to do with framework construction, the other with empirical verification. To be more precise, within software I am referring to the development of programming languages, libraries, and program architectures as framework construction, while watching our programs run, the use of debugging software, and testing comprise the empirical part. In science the frameworks developed, called theories, provide a language for describing observable phenomena; in software we use our frameworks to express descriptions, too—but the situation is different in that there is no physical phenomenon to describe. Instead, the object of our descriptions is the conception of a program, or maybe a written specification. The objects we’d like to describe with computer programs are more abstract, but play the same role as any physical phenomenon of interest in the sciences. That we are creating descriptions in software is somewhat disguised by the common necessity to use an imperative language, but this is incidental: our goal in writing programs is to precisely describe a conceived piece of software so that a computer may precisely generate it. If performance were not a concern, there’s no doubt we would only rarely use imperative languages, declarative languages being a better fit for the nature of the task.

		If software development is similar to the practice of science, then directions for improving our facility with it should result by considering what has been effective for science—and If there’s one important lesson science learned, before it earned its dignified title, it was to value empiricism. Writing software certainly involves a necessary empirical component. To deny this would be the same as saying that we never observe the programs we write under execution. But, the non-centrality of the observations we make is reminiscent of pre-scientific attempts at understanding the world: Plato couldn’t help but look outside, or for what he observed there to influence his thinking; and Aristotle certainly knew the importance of making observations—yet he wasn’t trying to build telescopes and pendulums, and he didn’t reap the benefits this endeavor might have led him to. In physical science, the emphasis on observation led to the development of instruments which enhance our observational capabilities by making automated, consistent transformations of ‘physical data.’ For example, using a microscope, light is transformed in such a way as to create a magnification of what we would observe without it. In software, if we put a similar emphasis on the importance of observation, it seems likely that we would develop similarly essential instruments to aid our work. At the moment we do have print statements, breakpoint debuggers, and ad hoc visual debuggers. These are an indication that we don’t completely spurn empiricism in software development—but our instruments are still rudimentary, and if we really cared about it we’d do better. For example, the most sophisticated tool in common use is probably the breakpoint debugger: it’s damn useful in certain scenarios, but it couldn’t be called essential—one can get by just fine with print statements, because it doesn’t help that much.

		Let’s look more closely at the role of observation-enhancing instruments in science. The act of making an observation could be viewed as sampling the current state of our subject; observation-enhancing instruments perform some consistent transformation of this state information, because the raw state data (e.g. photons bouncing off a petri dish) are unusable directly. The instruments we could develop for software development would fulfill a similar function, but the transformations of interest would be different. In software we already have the problem solved, which troubles physical scientists, of getting detailed, concrete measurements of our subject: we created the programs we’re attempting to inspect—if we just look at a dump of the computer’s memory, all the data is there. So maybe what our instruments should do, rather than letting us look more closely, is let us look more abstractly, in a consistent, automated fashion. When we write programs, we explicitly create differing levels of abstraction; can we not create instruments that allow us to make inspections at these various abstraction levels? If we were to do this, we could select representations of the data appropriate to the abstraction level under observation. I’ll bet, for instance, that our visual debuggers would be more effective if we visualized more abstract entities than single variable/function values.

		In science and in software, our empirical activities are useful only in combination with certain linguistic activities. In science this refers to the construction of theoretical frameworks and statements made within the frameworks; in software, it refers to the construction of programming languages and libraries, and the programs written using them. In both cases we’d like to be very precise in describing unfamiliar, complex entities, so we first develop a specialized language to make our statements in. However, these languages have always been imperfect and require revisions; in software we call this ‘refactoring,’ in science we have the (Kuhnian) revolution, or paradigm shift. Again, we have the situation where in software the scientific counterpart is known, but its perceived relation to the field is different. In science a paradigm shift is a mark of extraordinary progress—it means we’ve finally attained the clarity of perception to see ‘what’s really happening.’ Whereas in software, if we’re refactoring, there’s an attitude that a mistake has been made, and now we’re correcting it. (You can find the same attitude in the way we think of our observation-enhancing instruments: we’ve named them debuggers! The attitude, expressed with varying levels of awareness, is that a mistake has been made: the programmer created a bug, and now, because of their shortcoming, we must use a tool to remove the bug.) In science, if we can conceive of a more elegant framework to express our ideas in, it’s gold—the ultimate prize (2). 

		Identifying software frameworks with theory in science could also give us better direction in educating ourselves. For instance, in science, while still lacking any definite recipe for attaining great theories, there is at least a clear image of what a great theory would look like: it’s elegant—or, to be more precise, the ratio of the theory’s complexity to the complexity of what it describes is low. Plenty of individual software developers recognize that an elegant framework, in the same sense, is desirable—but how much attention does the standard CS curriculum give to this concept? Maybe it’s mentioned once or twice. Of course it’s not part of the standard undergraduate physics curriculum either, but this makes perfect sense since the proportion of physicists who need to construct theoretical frameworks is relatively small. In software however, this activity is required at least once for every software project—it’s the bread in the ‘bread and butter’ of our work, we do it constantly. Even when it would be typical to consider an architecture finished, at the point where one is ‘just writing code,’ we continue modifying the framework we’re working in with every new function definition.

		To see how lessons in framework construction from science and mathematics could carry over to software, let’s look at linear algebra—but not just how it’s used. Instead, if you consider why it’s successful, why the way it’s formulated works, you find nicely isolated instances of important principles. For instance, why is it desirable to have the construct of basis vectors that coordinates can be expressed in terms of? If you transform the basis vectors, any object in the space is transformed in the same way—makes it possible to encode transformations in matrices. My suspicion is that the start of this formulation was the insight that direction and magnitude could be separated, which allows you to treat the concepts separately, whereas Cartesian coordinates embody both concepts inextricably; hence we can view the situation as a refactoring of the Cartesian system. The history of science and mathematics is essentially a catalogue of such refactorings—and they are great illustrations of the sort of ‘design patterns’ we’d benefit by studying. In fact, I’d bet there’s more value to be gained by analyzing linear algebra in this way than, say, reading the Unix kernel source code. Maybe that was untrue even 20 years ago, but I believe it becomes increasingly true as our languages become more abstract (i.e. not tied to the form of the hardware we execute them on).

		I hope I’m not taken as saying there shouldn’t be an engineering discipline for software, however. There are clearly aspects of software development understood well enough to systematize into repeatable processes—but there are other aspects where we may be fooling ourselves in thinking we understand well enough to crystallize definite recommendations. Maybe what we’re missing is a scientific discipline whose results we could reliably apply—the trusted ground of established theory that software engineering’s more predictable siblings each stand on. If the structure of software development is something like it’s described here, the missing science would be Meta Science, a conversion of philosophy of science into science of science. Though, personally, while I think it’s possible something fruitful could emerge along those lines, it seems pretty unlikely. The connections between science and software development, however, do seem compelling. And if science turns out not to be a productive analogy, I still think we’d benefit by analyzing and better framing the general activity of human software development: improving our understanding would lead to improvements in the values and methods of software engineering.

		(1) I’m going to be using ‘science’ in a pretty narrow and particular sense throughout the essay. What I mean by it is the ‘essence’ of science, or something along those lines. More exactly, I mean partially systematic, adaptive theory construction by the cyclical process of hypothesis formulation, empirical confirmation or rejection, and theory adaption.

		(2) While I do think a new theory widely accepted and displacing an old is ‘the ultimate prize,’ the statement should probably be somewhat tempered by the very real resistance which the proposal of new candidate theories must inspire. In science and software, there’s a lot of work done in each field that is invalidated if the underlying framework is replaced—so we want to be very sure we’re ‘on the right track’ in implementing the replacement. Makes sense. I think, however, the attitude in software is typically more similar to getting oneself ‘out of a hole’ by a refactor, whereas science feels it was already in the black and the Christmas bonus has just rolled in. The psychological impacts of each of these scenarios is quite different, and unfortunate in the case of software. The process of ‘debugging’ has a similar detrimental psychological effect. And of course not every developer will view refactors and debugging like I describe, but this does seem to be the norm.

		—————————————
		software and science aren’t the same thing, they are each members of a class whose generator we can begin to understand, knowing two of it’s members. What does the generalization of science/software look like? We know the two things differ in their particulars, like software describes abstract entities, and performance is a concern. I believe the generalization is ‘description,’ construction of linguistic correspondences, the definition of (iso)morphisms from non-linguistic domains to linguistic domains. Also interesting, from the wikipedia page on category theory: ‘The most important property of the arrows is that they can be "composed", in other words, arranged in a sequence to form a new arrow.’

		It may be that the key to learning/teaching software well is to learn how to do abstraction ladder navigation. If that's the case, instructing people to solve software problems at the lowest level of abstraction only, namely with the use of some particular language, is a mistake. We should figure out more ideal ladder navigation patterns and teach in a way that emphasizes these patterns. For one thing, information hiding and 'hierarchical decomposition' should be taught in the abstract at some point, not incidentally through some particular language's implementation.

		You should try to come up with a more serious catalog of what these fundamental, abstract techniques are.

		Maybe another important thing comes from the chief programmer system: one person conceives the top level architecture (would we likely end up with a nice system like maxwell’s equations if instead of being one person’s conception a manager split the task among five people who would each be responsible for some aspect of electricity or magnetism?)

		“If, as I believe, the conceptual components of the task are now taking most of the time, then no amount of activity on the task components that are merely the expression of the concepts can give large productivity gains.
			Hence we must consider those attacks that address the essence of the software problem, the formulation of these complex conceptual structures. Fortunately some of these are very promising.”

		As soon as we started programming, we found to our surprise that it wasn’t as easy to get programs right as we had thought. Debugging had to be discovered. I can remember the exact instant when I realized that a large part of my life from then on was going to be spent in finding mistakes in my own programs.
		        — Maurice Wilkes discovers debugging, 1949
		————————————
		—————————————————————————————————

		we define an abstract framework which defines the terms with which we compose our particular formulations; in order to verify the utility of some formulation we take measurements of its products 

		Often in intellectual work, when faced with such intractable-seeming problems, we eventually discover that the excessive complexity was something added by our own perspective on the matter, rather than anything intrinsic. Unfortunately, I doubt this will be the case for software—or at least if it is the case, our change in perspective will at once sort out the difficulties in 

		especially when it seems the domain is intrinsically so complex and disordered that

		In science there tends to be a clear distinction between when one is developing a theoretical framework and when one is expressing statements within it; but programming languages (which are themselves part of the architecture of a software project) are a framework that unifies creating frameworks with making statements in the frameworks. In other words, when we write code, we are continually defining new classes, functions, and variables and the same time that we are using these constructs.

		And that jargon makes our science of not engineering very robust. This is the key here: we need to do robust science when we do our programming science. And that way, we can make stable, happy products that don’t suck. They’ll be, in fact, ROBUST !! That’s the secret, my programming friends: the jargon shall make the robust discipline of science that we so dearly need to make the very robust, stable, happy, products. The question of questions is answered! And it hath spoken. And what the answer of answers spaketh is ROBUSTNESS, my friends. And SCIENCE, my friends. These are the answers. The ultimate programing answers. If you call within the next fifteen minutes, you’ll get all the jargon you want at an extremely low introductory price! 

		If we consider software development’s ideal methodology to be more in line with that of science, there are some shifts in emphasis in dealing with architecture that we might observe. First, we should place high value on unifying reformulations of our program architectures. We should understand that we will almost certainly not ‘get it right’ initially; that the problem of finding an elegant formulation of some problem domain is among the most difficult of all human intellectual endeavors (again, see Wolfram), as it’s recognized to be in the sciences. At present there are developers who have experienced the stark difference between what I’d call natural and unnatural formulations, and the consequent complexities attendant to each—but many more who are suspicious that a ‘better formulation’ is ever worth throwing out code. 

		For instance, there’s an idea at the moment that we should read great code in order to understand how it’s done. However, while this is in line with a useful pedagogical concept, maybe there are other ways of implementing it. When reading a great piece of code we only see the final, crystalized product of a lot of dynamic intellectual work, which is an issue recognized in mathematical pedagogy: many mathematical works aim to be a perfect codification of the theory they describe, which in doing so disguise the rough beginnings and successive refinements that underly such work. Something more useful than reading the source for the Unix kernel would be a document describing it at descending levels of abstraction (e.g. http://www.amazon.com/The-Design-UNIX-Operating-System/dp/0132017997/); significantly better still, however, would be a document describing the succession of formulations in the authors’ minds, and how/why the final formulation was preferred over the competition. Since the availability of such documents is basically nil at present, however, we can look for analogues in science, and these are readily available: the history of science and mathematics is a catalogue of theoretical refinements and unifications which software developers could benefit from studying.

		To recap, I think the kind of problem that software is, is closer to that of science than that of an engineering discipline. Adopting this perspective, and looking at the lessons learned in the methodology of science, I conclude that we should place greater value on developing effective/natural frameworks, as well as more effective instruments for observing the current state of our programs. We should figure out ways of re-representing debugging tools and refactoring in software culture so that they are seen not as necessary evils, but as the very foundation that allows us to do our work; and we should recognize that developing frameworks for software development isn’t a fundamentally new activity: the history of science and mathematics has a rich literature that we would do well to learn from.

		What’s the ideal methodology then, since there is clearly an engineering aspect to all this as well? I guess it’s trying to find out how to automate the creation of new scientific fields. Something less than that: in figuring out how to improve software engineering, knowing the nature of the beast is a good starting point; in other words considering software development to be a science is a good framework for thinking about what would be effective engineering values/methods. Another possibility is that: engineering disciplines are controlled applications of some scientific discipline; what is the scientific discipline that software engineering would apply in a controlled fashion? I think it’d look like this: if philosophy of science were a science, then software engineers would apply its results in order to construct particular pieces of software. We’ve jumped the gun in software engineering, trying to create the engineering discipline without first having the science it’s based on. We’ve already seen this sort of self-referential development in physics where the concept of making an observation is encoded in the theory. Testing is an example of software’s distinct methodological requirements.

		The two places of most striking dissimilarity that I see between the disciplines each start with values. Science values empiricism; software sees it as a necessary evil. Science values and respects a well-fitting framework, and it understands the difficulty involved in attaining one; software recognizes that its programs must stand on ‘something’ I’ll next treat each of these as they relate to software.
		Considering the current disparities between software and and scientific methodology in order to better accord with what wisdom we’ve gained in science. The first is that we should be more empirical, and the second that we should adopt a proper respect for the value and difficulty of creating a well-fitting framework. 

		 Or, if science is an appropriate analogy, but the use I’ve been outlining for the connection is misguided, then maybe the similarity just means software engineering needs to be less rigid, and that the software industry needs to embrace its own unpredictability—promising only that “it’s ready when it’s ready,” like Blizzard, or science—without feeling bad about it.

		Maybe it seems strange that a mathematician’s opinion on the ontological status of general mathematical objects could practically impact her work, but when we seriously adopt perspectives, they become part of the framework we think with.

		Maybe we’d be better off in software if we had something like the notion of truth in science: if we thought there were some ‘true’ formulation out there and at any given point in time, maybe we’ve got it—and maybe it remains to be discovered by some fortunate developer. We see a situation analogous to this in mathematics, where many mathematicians adopt the viewpoint that abstract mathematical objects have an existence independent of human minds. And the working hypothesis of a discoverable objective truth is clearly of fundamental importance in science.
	</body>
</html>
